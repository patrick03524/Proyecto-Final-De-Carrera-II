{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Propuesta \"*Alta Resolución Generative Adversarial Network*\"\n",
        "## Alumno: Patrick Xavier Marquez Choque\n",
        "## Curso: Proyecto Final de Carrera II\n",
        "## Periodo: 2023-I"
      ],
      "metadata": {
        "id": "BLHjct3ZTHXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Conv2DTranspose, LeakyReLU, BatchNormalization, Reshape, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Definir el generador\n",
        "def build_generator():\n",
        "    generator = Sequential([\n",
        "        Dense(4 * 4 * 1024, input_shape=(100,)),\n",
        "        Reshape((4, 4, 1024)),\n",
        "        Conv2DTranspose(512, (5, 5), strides=(2, 2), padding='same', activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', activation='tanh')\n",
        "    ])\n",
        "    return generator\n",
        "\n",
        "# Definir el discriminador\n",
        "def build_discriminator():\n",
        "    discriminator = Sequential([\n",
        "        Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=(512, 512, 3)),\n",
        "        LeakyReLU(0.2),\n",
        "        Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        Conv2D(256, (5, 5), strides=(2, 2), padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        Conv2D(512, (5, 5), strides=(2, 2), padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(0.2),\n",
        "        Flatten(),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return discriminator\n",
        "\n",
        "# Construir el generador y el discriminador\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "# Definir los hiperparámetros\n",
        "batch_size = 64\n",
        "latent_dim = 100\n",
        "epochs = 100\n",
        "\n",
        "# Definir la función de pérdida y los optimizadores\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "\n",
        "# Definir la función de entrenamiento\n",
        "@tf.function\n",
        "def train_step(real_images):\n",
        "    # Generar ruido aleatorio\n",
        "    noise = tf.random.normal((batch_size, latent_dim))\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        # Generar imágenes falsas con el generador\n",
        "        fake_images = generator(noise, training=True)\n",
        "\n",
        "        # Calcular la pérdida del discriminador en imágenes reales y falsas\n",
        "        real_output = discriminator(real_images, training=True)\n",
        "        fake_output = discriminator(fake_images, training=True)\n",
        "        gen_loss = loss_fn(tf.ones_like(fake_output), fake_output)\n",
        "        disc_loss = loss_fn(tf.ones_like(real_output), real_output) + loss_fn(tf.zeros_like(fake_output), fake_output)\n",
        "\n",
        "    # Calcular los gradientes y aplicar las actualizaciones\n",
        "    gradients_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    generator_optimizer.apply_gradients(zip(gradients_gen, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_disc, discriminator.trainable_variables))\n",
        "\n",
        "# Cargar el dataset CelebA (Asegúrate de tener los archivos de imágenes en la carpeta \"./celeba/img_align_celeba\")\n",
        "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"./celeba\",\n",
        "    image_size=(512, 512),\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123\n",
        ")\n",
        "\n",
        "# Preprocesar el dataset y definir el número de pasos por época\n",
        "dataset = dataset.map(lambda x: x / 255.0)\n",
        "steps_per_epoch = len(dataset) // batch_size\n",
        "\n",
        "# Entrenar el modelo\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    for step, real_images in enumerate(dataset):\n",
        "        train_step(real_images)\n",
        "\n",
        "    # Guardar imágenes generadas\n",
        "    random_noise = tf.random.normal((16, latent_dim))\n",
        "    generated_images = generator(random_noise, training=False)\n",
        "    generated_images = (generated_images + 1) * 0.5  # Escalar de -1 a 1 a 0 a 1\n",
        "    for i in range(generated_images.shape[0]):\n",
        "        image = tf.cast(generated_images[i] * 255, tf.uint8)\n",
        "        tf.keras.preprocessing.image.save_img(f\"generated_images/image_{epoch}_{i}.png\", image)\n"
      ],
      "metadata": {
        "id": "bhRfX0-AXcZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_psnr(image1, image2):\n",
        "    mse = np.mean((image1 - image2) ** 2)\n",
        "    max_value = np.max(image1)\n",
        "    psnr = 20 * np.log10(max_value / np.sqrt(mse))\n",
        "    return psnr\n"
      ],
      "metadata": {
        "id": "7gRld4ZWZRIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psnr_values = []\n",
        "for i in range(num_images):\n",
        "    psnr = calculate_psnr(original_images[i], generated_images[i])\n",
        "    psnr_values.append(psnr)\n",
        "    print(f\"PSNR for image {i+1}: {psnr} dB\")\n",
        "\n",
        "average_psnr = np.mean(psnr_values)\n",
        "print(f\"Average PSNR: {average_psnr} dB\")\n"
      ],
      "metadata": {
        "id": "Df6Lfo9aZTa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from skimage.metrics import peak_signal_noise_ratio\n",
        "\n",
        "def calculate_psnr(original_images, generated_images):\n",
        "    psnr_values = []\n",
        "    for i in range(len(original_images)):\n",
        "        psnr = peak_signal_noise_ratio(original_images[i], generated_images[i], data_range=1.0)\n",
        "        psnr_values.append(psnr)\n",
        "        print(f\"PSNR for image {i+1}: {psnr} dB\")\n",
        "\n",
        "    average_psnr = np.mean(psnr_values)\n",
        "    print(f\"Average PSNR: {average_psnr} dB\")\n",
        "\n",
        "# Supongamos que tienes las imágenes originales cargadas en la variable 'original_images'\n",
        "# y las imágenes generadas en la variable 'generated_images'\n",
        "\n",
        "# Carga las imágenes originales de CIFAR-10\n",
        "# (reemplaza 'load_cifar10' con el código necesario para cargar el conjunto de datos)\n",
        "original_images = load_cifar10()\n",
        "\n",
        "# Calcula el PSNR\n",
        "calculate_psnr(original_images, generated_images)"
      ],
      "metadata": {
        "id": "0wYa8-N5bgHj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}