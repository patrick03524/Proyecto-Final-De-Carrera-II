{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-XSb4-4LNuo"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.datasets import cifar10, mnist\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageData:\n",
        "\n",
        "    def __init__(self, load_size, channels, custom_dataset):\n",
        "        self.load_size = load_size\n",
        "        self.channels = channels\n",
        "        self.custom_dataset = custom_dataset\n",
        "\n",
        "    def image_processing(self, filename):\n",
        "\n",
        "        if not self.custom_dataset :\n",
        "            x_decode = filename\n",
        "        else :\n",
        "            x = tf.read_file(filename)\n",
        "            x_decode = tf.image.decode_jpeg(x, channels=self.channels)\n",
        "\n",
        "        img = tf.image.resize_images(x_decode, [self.load_size, self.load_size])\n",
        "        img = tf.cast(img, tf.float32) / 127.5 - 1\n",
        "\n",
        "        return img\n",
        "\n",
        "def load_mnist():\n",
        "    (train_data, train_labels), (test_data, test_labels) = mnist.load_data()\n",
        "    x = np.concatenate((train_data, test_data), axis=0)\n",
        "    x = np.expand_dims(x, axis=-1)\n",
        "\n",
        "    return x\n",
        "\n",
        "def load_cifar10() :\n",
        "    (train_data, train_labels), (test_data, test_labels) = cifar10.load_data()\n",
        "    x = np.concatenate((train_data, test_data), axis=0)\n",
        "\n",
        "    return x\n",
        "\n",
        "def load_data(dataset_name) :\n",
        "    if dataset_name == 'mnist' :\n",
        "        x = load_mnist()\n",
        "    elif dataset_name == 'cifar10' :\n",
        "        x = load_cifar10()\n",
        "    else :\n",
        "\n",
        "        x = glob(os.path.join(\"./dataset\", dataset_name, '*.*'))\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "8RssffhlNKDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(x, size):\n",
        "    x = scipy.misc.imread(x, mode='RGB')\n",
        "    x = scipy.misc.imresize(x, [size, size])\n",
        "    x = normalize(x)\n",
        "    return x\n",
        "\n",
        "def normalize(x) :\n",
        "    return x/127.5 - 1\n",
        "\n",
        "def save_images(images, size, image_path):\n",
        "    return imsave(inverse_transform(images), size, image_path)\n",
        "\n",
        "def merge(images, size):\n",
        "    h, w = images.shape[1], images.shape[2]\n",
        "    if (images.shape[3] in (3,4)):\n",
        "        c = images.shape[3]\n",
        "        img = np.zeros((h * size[0], w * size[1], c))\n",
        "        for idx, image in enumerate(images):\n",
        "            i = idx % size[1]\n",
        "            j = idx // size[1]\n",
        "            img[j * h:j * h + h, i * w:i * w + w, :] = image\n",
        "        return img\n",
        "    elif images.shape[3]==1:\n",
        "        img = np.zeros((h * size[0], w * size[1]))\n",
        "        for idx, image in enumerate(images):\n",
        "            i = idx % size[1]\n",
        "            j = idx // size[1]\n",
        "            img[j * h:j * h + h, i * w:i * w + w] = image[:,:,0]\n",
        "        return img\n",
        "    else:\n",
        "        raise ValueError('in merge(images,size) images parameter ''must have dimensions: HxW or HxWx3 or HxWx4')\n",
        "\n",
        "def imsave(images, size, path):\n",
        "    # image = np.squeeze(merge(images, size)) # 채널이 1인거 제거 ?\n",
        "    return scipy.misc.imsave(path, merge(images, size))\n",
        "\n",
        "\n",
        "def inverse_transform(images):\n",
        "    return (images+1.)/2.\n",
        "\n",
        "\n",
        "def check_folder(log_dir):\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "    return log_dir\n",
        "\n",
        "def show_all_variables():\n",
        "    model_vars = tf.trainable_variables()\n",
        "    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n",
        "\n",
        "def str2bool(x):\n",
        "    return x.lower() in ('true')"
      ],
      "metadata": {
        "id": "JoV-1oYdNqNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def orthogonal_regularizer(scale) :\n",
        "    \"\"\" Defining the Orthogonal regularizer and return the function at last to be used in Conv layer as kernel regularizer\"\"\"\n",
        "\n",
        "    def ortho_reg(w) :\n",
        "        \"\"\" Reshaping the matrxi in to 2D tensor for enforcing orthogonality\"\"\"\n",
        "        _, _, _, c = w.get_shape().as_list()\n",
        "\n",
        "        w = tf.reshape(w, [-1, c])\n",
        "\n",
        "        \"\"\" Declaring a Identity Tensor of appropriate size\"\"\"\n",
        "        identity = tf.eye(c)\n",
        "\n",
        "        \"\"\" Regularizer Wt*W - I \"\"\"\n",
        "        w_transpose = tf.transpose(w)\n",
        "        w_mul = tf.matmul(w_transpose, w)\n",
        "        reg = tf.subtract(w_mul, identity)\n",
        "\n",
        "        \"\"\"Calculating the Loss Obtained\"\"\"\n",
        "        ortho_loss = tf.nn.l2_loss(reg)\n",
        "\n",
        "        return scale * ortho_loss\n",
        "\n",
        "    return ortho_reg\n",
        "\n",
        "def orthogonal_regularizer_fully(scale) :\n",
        "    \"\"\" Defining the Orthogonal regularizer and return the function at last to be used in Fully Connected Layer \"\"\"\n",
        "\n",
        "    def ortho_reg_fully(w) :\n",
        "        \"\"\" Reshaping the matrix in to 2D tensor for enforcing orthogonality\"\"\"\n",
        "        _, c = w.get_shape().as_list()\n",
        "\n",
        "        \"\"\"Declaring a Identity Tensor of appropriate size\"\"\"\n",
        "        identity = tf.eye(c)\n",
        "        w_transpose = tf.transpose(w)\n",
        "        w_mul = tf.matmul(w_transpose, w)\n",
        "        reg = tf.subtract(w_mul, identity)\n",
        "\n",
        "        \"\"\" Calculating the Loss \"\"\"\n",
        "        ortho_loss = tf.nn.l2_loss(reg)\n",
        "\n",
        "        return scale * ortho_loss\n",
        "\n",
        "    return ortho_reg_fully"
      ],
      "metadata": {
        "id": "Rhk19jk5Nwey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv(x, channels, kernel=4, stride=2, pad=0, pad_type='zero', use_bias=True, sn=False, scope='conv_0'):\n",
        "    with tf.variable_scope(scope):\n",
        "        if pad > 0:\n",
        "            h = x.get_shape().as_list()[1]\n",
        "            if h % stride == 0:\n",
        "                pad = pad * 2\n",
        "            else:\n",
        "                pad = max(kernel - (h % stride), 0)\n",
        "\n",
        "            pad_top = pad // 2\n",
        "            pad_bottom = pad - pad_top\n",
        "            pad_left = pad // 2\n",
        "            pad_right = pad - pad_left\n",
        "\n",
        "            if pad_type == 'zero' :\n",
        "                x = tf.pad(x, [[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]])\n",
        "            if pad_type == 'reflect' :\n",
        "                x = tf.pad(x, [[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]], mode='REFLECT')\n",
        "\n",
        "        if sn :\n",
        "            if scope.__contains__('generator') :\n",
        "                w = tf.get_variable(\"kernel\", shape=[kernel, kernel, x.get_shape()[-1], channels], initializer=weight_init,\n",
        "                                    regularizer=weight_regularizer)\n",
        "            else :\n",
        "                w = tf.get_variable(\"kernel\", shape=[kernel, kernel, x.get_shape()[-1], channels], initializer=weight_init,\n",
        "                                    regularizer=None)\n",
        "\n",
        "            x = tf.nn.conv2d(input=x, filter=spectral_norm(w),\n",
        "                             strides=[1, stride, stride, 1], padding='VALID')\n",
        "            if use_bias :\n",
        "                bias = tf.get_variable(\"bias\", [channels], initializer=tf.constant_initializer(0.0))\n",
        "                x = tf.nn.bias_add(x, bias)\n",
        "\n",
        "        else :\n",
        "            if scope.__contains__('generator'):\n",
        "                x = tf.layers.conv2d(inputs=x, filters=channels,\n",
        "                                     kernel_size=kernel, kernel_initializer=weight_init,\n",
        "                                     kernel_regularizer=weight_regularizer,\n",
        "                                     strides=stride, use_bias=use_bias)\n",
        "            else :\n",
        "                x = tf.layers.conv2d(inputs=x, filters=channels,\n",
        "                                     kernel_size=kernel, kernel_initializer=weight_init,\n",
        "                                     kernel_regularizer=None,\n",
        "                                     strides=stride, use_bias=use_bias)\n",
        "\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "FlF7RM4uNzlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deconv(x, channels, kernel=4, stride=2, padding='SAME', use_bias=True, sn=False, scope='deconv_0'):\n",
        "    with tf.variable_scope(scope):\n",
        "        x_shape = x.get_shape().as_list()\n",
        "\n",
        "        if padding == 'SAME':\n",
        "            output_shape = [x_shape[0], x_shape[1] * stride, x_shape[2] * stride, channels]\n",
        "\n",
        "        else:\n",
        "            output_shape =[x_shape[0], x_shape[1] * stride + max(kernel - stride, 0), x_shape[2] * stride + max(kernel - stride, 0), channels]\n",
        "\n",
        "        if sn :\n",
        "            w = tf.get_variable(\"kernel\", shape=[kernel, kernel, channels, x.get_shape()[-1]], initializer=weight_init, regularizer=weight_regularizer)\n",
        "            x = tf.nn.conv2d_transpose(x, filter=spectral_norm(w), output_shape=output_shape, strides=[1, stride, stride, 1], padding=padding)\n",
        "\n",
        "            if use_bias :\n",
        "                bias = tf.get_variable(\"bias\", [channels], initializer=tf.constant_initializer(0.0))\n",
        "                x = tf.nn.bias_add(x, bias)\n",
        "\n",
        "        else :\n",
        "            x = tf.layers.conv2d_transpose(inputs=x, filters=channels,\n",
        "                                           kernel_size=kernel, kernel_initializer=weight_init, kernel_regularizer=weight_regularizer,\n",
        "                                           strides=stride, padding=padding, use_bias=use_bias)\n",
        "\n",
        "        return x\n",
        "\n",
        "def fully_conneted(x, units, use_bias=True, sn=False, scope='fully_0'):\n",
        "    with tf.variable_scope(scope):\n",
        "        x = flatten(x)\n",
        "        shape = x.get_shape().as_list()\n",
        "        channels = shape[-1]\n",
        "\n",
        "        if sn :\n",
        "            if scope.__contains__('generator'):\n",
        "                w = tf.get_variable(\"kernel\", [channels, units], tf.float32, initializer=weight_init, regularizer=weight_regularizer_fully)\n",
        "            else :\n",
        "                w = tf.get_variable(\"kernel\", [channels, units], tf.float32, initializer=weight_init, regularizer=None)\n",
        "\n",
        "            if use_bias :\n",
        "                bias = tf.get_variable(\"bias\", [units], initializer=tf.constant_initializer(0.0))\n",
        "\n",
        "                x = tf.matmul(x, spectral_norm(w)) + bias\n",
        "            else :\n",
        "                x = tf.matmul(x, spectral_norm(w))\n",
        "\n",
        "        else :\n",
        "            if scope.__contains__('generator'):\n",
        "                x = tf.layers.dense(x, units=units, kernel_initializer=weight_init,\n",
        "                                    kernel_regularizer=weight_regularizer_fully, use_bias=use_bias)\n",
        "            else :\n",
        "                x = tf.layers.dense(x, units=units, kernel_initializer=weight_init,\n",
        "                                    kernel_regularizer=None, use_bias=use_bias)\n",
        "\n",
        "        return x\n",
        "\n",
        "def flatten(x) :\n",
        "    return tf.layers.flatten(x)\n",
        "\n",
        "def hw_flatten(x) :\n",
        "    return tf.reshape(x, shape=[x.shape[0], -1, x.shape[-1]])\n"
      ],
      "metadata": {
        "id": "8Wx4jP9gN5ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resblock(x_init, channels, use_bias=True, is_training=True, sn=False, scope='resblock'):\n",
        "    with tf.variable_scope(scope):\n",
        "        with tf.variable_scope('res1'):\n",
        "            x = conv(x_init, channels, kernel=3, stride=1, pad=1, use_bias=use_bias, sn=sn)\n",
        "            x = batch_norm(x, is_training)\n",
        "            x = relu(x)\n",
        "\n",
        "        with tf.variable_scope('res2'):\n",
        "            x = conv(x, channels, kernel=3, stride=1, pad=1, use_bias=use_bias, sn=sn)\n",
        "            x = batch_norm(x, is_training)\n",
        "\n",
        "        return x + x_init\n",
        "\n",
        "def resblock_up(x_init, channels, use_bias=True, is_training=True, sn=False, scope='resblock_up'):\n",
        "    with tf.variable_scope(scope):\n",
        "        with tf.variable_scope('res1'):\n",
        "            x = batch_norm(x_init, is_training)\n",
        "            x = relu(x)\n",
        "            x = deconv(x, channels, kernel=3, stride=2, use_bias=use_bias, sn=sn)\n",
        "\n",
        "        with tf.variable_scope('res2') :\n",
        "            x = batch_norm(x, is_training)\n",
        "            x = relu(x)\n",
        "            x = deconv(x, channels, kernel=3, stride=1, use_bias=use_bias, sn=sn)\n",
        "\n",
        "        with tf.variable_scope('skip') :\n",
        "            x_init = deconv(x_init, channels, kernel=3, stride=2, use_bias=use_bias, sn=sn)\n",
        "\n",
        "\n",
        "    return x + x_init\n",
        "\n",
        "def resblock_up_condition(x_init, z, channels, use_bias=True, is_training=True, sn=False, scope='resblock_up'):\n",
        "    with tf.variable_scope(scope):\n",
        "        with tf.variable_scope('res1'):\n",
        "            x = condition_batch_norm(x_init, z, is_training)\n",
        "            x = relu(x)\n",
        "            x = deconv(x, channels, kernel=3, stride=2, use_bias=use_bias, sn=sn)\n",
        "\n",
        "        with tf.variable_scope('res2') :\n",
        "            x = condition_batch_norm(x, z, is_training)\n",
        "            x = relu(x)\n",
        "            x = deconv(x, channels, kernel=3, stride=1, use_bias=use_bias, sn=sn)\n",
        "\n",
        "        with tf.variable_scope('skip') :\n",
        "            x_init = deconv(x_init, channels, kernel=3, stride=2, use_bias=use_bias, sn=sn)\n",
        "\n",
        "\n",
        "    return x + x_init\n",
        "\n",
        "\n",
        "def resblock_down(x_init, channels, use_bias=True, is_training=True, sn=False, scope='resblock_down'):\n",
        "    with tf.variable_scope(scope):\n",
        "        with tf.variable_scope('res1'):\n",
        "            x = batch_norm(x_init, is_training)\n",
        "            x = relu(x)\n",
        "            x = conv(x, channels, kernel=3, stride=2, pad=1, use_bias=use_bias, sn=sn)\n",
        "\n",
        "        with tf.variable_scope('res2') :\n",
        "            x = batch_norm(x, is_training)\n",
        "            x = relu(x)\n",
        "            x = conv(x, channels, kernel=3, stride=1, pad=1, use_bias=use_bias, sn=sn)\n",
        "\n",
        "        with tf.variable_scope('skip') :\n",
        "            x_init = conv(x_init, channels, kernel=3, stride=2, pad=1, use_bias=use_bias, sn=sn)\n",
        "\n",
        "\n",
        "    return x + x_init"
      ],
      "metadata": {
        "id": "efmjG8WtN9cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def self_attention(x, channels, sn=False, scope='self_attention'):\n",
        "    with tf.variable_scope(scope):\n",
        "        f = conv(x, channels // 8, kernel=1, stride=1, sn=sn, scope='f_conv')  # [bs, h, w, c']\n",
        "        g = conv(x, channels // 8, kernel=1, stride=1, sn=sn, scope='g_conv')  # [bs, h, w, c']\n",
        "        h = conv(x, channels, kernel=1, stride=1, sn=sn, scope='h_conv')  # [bs, h, w, c]\n",
        "\n",
        "        # N = h * w\n",
        "        s = tf.matmul(hw_flatten(g), hw_flatten(f), transpose_b=True)  # # [bs, N, N]\n",
        "\n",
        "        beta = tf.nn.softmax(s)  # attention map\n",
        "\n",
        "        o = tf.matmul(beta, hw_flatten(h))  # [bs, N, C]\n",
        "        gamma = tf.get_variable(\"gamma\", [1], initializer=tf.constant_initializer(0.0))\n",
        "\n",
        "        o = tf.reshape(o, shape=x.shape)  # [bs, h, w, C]\n",
        "        x = gamma * o + x\n",
        "\n",
        "    return x\n",
        "\n",
        "def self_attention_2(x, channels, sn=False, scope='self_attention'):\n",
        "    with tf.variable_scope(scope):\n",
        "        f = conv(x, channels // 8, kernel=1, stride=1, sn=sn, scope='f_conv')  # [bs, h, w, c']\n",
        "        f = max_pooling(f)\n",
        "\n",
        "        g = conv(x, channels // 8, kernel=1, stride=1, sn=sn, scope='g_conv')  # [bs, h, w, c']\n",
        "\n",
        "        h = conv(x, channels // 2, kernel=1, stride=1, sn=sn, scope='h_conv')  # [bs, h, w, c]\n",
        "        h = max_pooling(h)\n",
        "\n",
        "        # N = h * w\n",
        "        s = tf.matmul(hw_flatten(g), hw_flatten(f), transpose_b=True)  # # [bs, N, N]\n",
        "\n",
        "        beta = tf.nn.softmax(s)  # attention map\n",
        "\n",
        "        o = tf.matmul(beta, hw_flatten(h))  # [bs, N, C]\n",
        "        gamma = tf.get_variable(\"gamma\", [1], initializer=tf.constant_initializer(0.0))\n",
        "\n",
        "        o = tf.reshape(o, shape=[x.shape[0], x.shape[1], x.shape[2], channels // 2])  # [bs, h, w, C]\n",
        "        o = conv(o, channels, kernel=1, stride=1, sn=sn, scope='attn_conv')\n",
        "        x = gamma * o + x\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "aMcSTlVaOBtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def global_avg_pooling(x):\n",
        "    gap = tf.reduce_mean(x, axis=[1, 2])\n",
        "\n",
        "    return gap\n",
        "\n",
        "def global_sum_pooling(x) :\n",
        "    gsp = tf.reduce_sum(x, axis=[1, 2])\n",
        "\n",
        "    return gsp\n",
        "\n",
        "def max_pooling(x) :\n",
        "    x = tf.layers.max_pooling2d(x, pool_size=2, strides=2, padding='SAME')\n",
        "    return x\n",
        "\n",
        "def up_sample(x, scale_factor=2):\n",
        "    _, h, w, _ = x.get_shape().as_list()\n",
        "    new_size = [h * scale_factor, w * scale_factor]\n",
        "    return tf.image.resize_nearest_neighbor(x, size=new_size)"
      ],
      "metadata": {
        "id": "f6KO1GEGOFG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lrelu(x, alpha=0.2):\n",
        "    return tf.nn.leaky_relu(x, alpha)\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def tanh(x):\n",
        "    return tf.tanh(x)"
      ],
      "metadata": {
        "id": "sOea_zB2OHfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_norm(x, is_training=True, scope='batch_norm'):\n",
        "    return tf.layers.batch_normalization(x,\n",
        "                                         momentum=0.9,\n",
        "                                         epsilon=1e-05,\n",
        "                                         training=is_training,\n",
        "                                         name=scope)\n",
        "\n",
        "def condition_batch_norm(x, z, is_training=True, scope='batch_norm'):\n",
        "    with tf.variable_scope(scope) :\n",
        "        _, _, _, c = x.get_shape().as_list()\n",
        "        decay = 0.9\n",
        "        epsilon = 1e-05\n",
        "\n",
        "        test_mean = tf.get_variable(\"pop_mean\", shape=[c], dtype=tf.float32, initializer=tf.constant_initializer(0.0), trainable=False)\n",
        "        test_var = tf.get_variable(\"pop_var\", shape=[c], dtype=tf.float32, initializer=tf.constant_initializer(1.0), trainable=False)\n",
        "\n",
        "        beta = fully_conneted(z, units=c, scope='beta')\n",
        "        gamma = fully_conneted(z, units=c, scope='gamma')\n",
        "\n",
        "        beta = tf.reshape(beta, shape=[-1, 1, 1, c])\n",
        "        gamma = tf.reshape(gamma, shape=[-1, 1, 1, c])\n",
        "\n",
        "        if is_training:\n",
        "            batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2])\n",
        "            ema_mean = tf.assign(test_mean, test_mean * decay + batch_mean * (1 - decay))\n",
        "            ema_var = tf.assign(test_var, test_var * decay + batch_var * (1 - decay))\n",
        "\n",
        "            with tf.control_dependencies([ema_mean, ema_var]):\n",
        "                return tf.nn.batch_normalization(x, batch_mean, batch_var, beta, gamma, epsilon)\n",
        "        else:\n",
        "            return tf.nn.batch_normalization(x, test_mean, test_var, beta, gamma, epsilon)\n",
        "\n",
        "\n",
        "def spectral_norm(w, iteration=1):\n",
        "    w_shape = w.shape.as_list()\n",
        "    w = tf.reshape(w, [-1, w_shape[-1]])\n",
        "\n",
        "    u = tf.get_variable(\"u\", [1, w_shape[-1]], initializer=tf.random_normal_initializer(), trainable=False)\n",
        "\n",
        "    u_hat = u\n",
        "    v_hat = None\n",
        "    for i in range(iteration):\n",
        "        \"\"\"\n",
        "        power iteration\n",
        "        Usually iteration = 1 will be enough\n",
        "        \"\"\"\n",
        "\n",
        "        v_ = tf.matmul(u_hat, tf.transpose(w))\n",
        "        v_hat = tf.nn.l2_normalize(v_)\n",
        "\n",
        "        u_ = tf.matmul(v_hat, w)\n",
        "        u_hat = tf.nn.l2_normalize(u_)\n",
        "\n",
        "    u_hat = tf.stop_gradient(u_hat)\n",
        "    v_hat = tf.stop_gradient(v_hat)\n",
        "\n",
        "    sigma = tf.matmul(tf.matmul(v_hat, w), tf.transpose(u_hat))\n",
        "\n",
        "    with tf.control_dependencies([u.assign(u_hat)]):\n",
        "        w_norm = w / sigma\n",
        "        w_norm = tf.reshape(w_norm, w_shape)\n",
        "\n",
        "    return w_norm\n"
      ],
      "metadata": {
        "id": "VUbsFG5QOKvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def discriminator_loss(loss_func, real, fake):\n",
        "    real_loss = 0\n",
        "    fake_loss = 0\n",
        "\n",
        "    if loss_func.__contains__('wgan') :\n",
        "        real_loss = -tf.reduce_mean(real)\n",
        "        fake_loss = tf.reduce_mean(fake)\n",
        "\n",
        "    if loss_func == 'lsgan' :\n",
        "        real_loss = tf.reduce_mean(tf.squared_difference(real, 1.0))\n",
        "        fake_loss = tf.reduce_mean(tf.square(fake))\n",
        "\n",
        "    if loss_func == 'gan' or loss_func == 'dragan' :\n",
        "        real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(real), logits=real))\n",
        "        fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(fake), logits=fake))\n",
        "\n",
        "    if loss_func == 'hinge' :\n",
        "        real_loss = tf.reduce_mean(relu(1.0 - real))\n",
        "        fake_loss = tf.reduce_mean(relu(1.0 + fake))\n",
        "\n",
        "    loss = real_loss + fake_loss\n",
        "\n",
        "    return loss\n",
        "\n",
        "def generator_loss(loss_func, fake):\n",
        "    fake_loss = 0\n",
        "\n",
        "    if loss_func.__contains__('wgan') :\n",
        "        fake_loss = -tf.reduce_mean(fake)\n",
        "\n",
        "    if loss_func == 'lsgan' :\n",
        "        fake_loss = tf.reduce_mean(tf.squared_difference(fake, 1.0))\n",
        "\n",
        "    if loss_func == 'gan' or loss_func == 'dragan' :\n",
        "        fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(fake), logits=fake))\n",
        "\n",
        "    if loss_func == 'hinge' :\n",
        "        fake_loss = -tf.reduce_mean(fake)\n",
        "\n",
        "    loss = fake_loss\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "B9J__rxJONq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigGAN_128(object):\n",
        "\n",
        "    def __init__(self, sess, args):\n",
        "        self.model_name = \"BigGAN\"  # name for checkpoint\n",
        "        self.sess = sess\n",
        "        self.dataset_name = args.dataset\n",
        "        self.checkpoint_dir = args.checkpoint_dir\n",
        "        self.sample_dir = args.sample_dir\n",
        "        self.result_dir = args.result_dir\n",
        "        self.log_dir = args.log_dir\n",
        "\n",
        "        self.epoch = args.epoch\n",
        "        self.iteration = args.iteration\n",
        "        self.batch_size = args.batch_size\n",
        "        self.print_freq = args.print_freq\n",
        "        self.save_freq = args.save_freq\n",
        "        self.img_size = args.img_size\n",
        "\n",
        "        \"\"\" Generator \"\"\"\n",
        "        self.ch = args.ch\n",
        "        self.z_dim = args.z_dim  # dimension of noise-vector\n",
        "        self.gan_type = args.gan_type\n",
        "\n",
        "        \"\"\" Discriminator \"\"\"\n",
        "        self.n_critic = args.n_critic\n",
        "        self.sn = args.sn\n",
        "        self.ld = args.ld\n",
        "\n",
        "        self.sample_num = args.sample_num  # number of generated images to be saved\n",
        "        self.test_num = args.test_num\n",
        "\n",
        "        # train\n",
        "        self.g_learning_rate = args.g_lr\n",
        "        self.d_learning_rate = args.d_lr\n",
        "        self.beta1 = args.beta1\n",
        "        self.beta2 = args.beta2\n",
        "        self.moving_decay = args.moving_decay\n",
        "\n",
        "        self.custom_dataset = False\n",
        "\n",
        "        if self.dataset_name == 'mnist':\n",
        "            self.c_dim = 1\n",
        "            self.data = load_mnist()\n",
        "\n",
        "        elif self.dataset_name == 'cifar10':\n",
        "            self.c_dim = 3\n",
        "            self.data = load_cifar10()\n",
        "\n",
        "        else:\n",
        "            self.c_dim = 3\n",
        "            self.data = load_data(dataset_name=self.dataset_name)\n",
        "            self.custom_dataset = True\n",
        "\n",
        "        self.dataset_num = len(self.data)\n",
        "\n",
        "        self.sample_dir = os.path.join(self.sample_dir, self.model_dir)\n",
        "        check_folder(self.sample_dir)\n",
        "\n",
        "        print()\n",
        "\n",
        "        print(\"##### Information #####\")\n",
        "        print(\"# BigGAN 128\")\n",
        "        print(\"# gan type : \", self.gan_type)\n",
        "        print(\"# dataset : \", self.dataset_name)\n",
        "        print(\"# dataset number : \", self.dataset_num)\n",
        "        print(\"# batch_size : \", self.batch_size)\n",
        "        print(\"# epoch : \", self.epoch)\n",
        "        print(\"# iteration per epoch : \", self.iteration)\n",
        "\n",
        "        print()\n",
        "\n",
        "        print(\"##### Generator #####\")\n",
        "        print(\"# spectral normalization : \", self.sn)\n",
        "        print(\"# learning rate : \", self.g_learning_rate)\n",
        "\n",
        "        print()\n",
        "\n",
        "        print(\"##### Discriminator #####\")\n",
        "        print(\"# the number of critic : \", self.n_critic)\n",
        "        print(\"# spectral normalization : \", self.sn)\n",
        "        print(\"# learning rate : \", self.d_learning_rate)\n",
        "\n",
        "    ##################################################################################\n",
        "    # Generator\n",
        "    ##################################################################################\n",
        "\n",
        "    def generator(self, z, is_training=True, reuse=False):\n",
        "        with tf.variable_scope(\"generator\", reuse=reuse):\n",
        "            # 6\n",
        "            if self.z_dim == 128:\n",
        "                split_dim = 20\n",
        "                split_dim_remainder = self.z_dim - (split_dim * 5)\n",
        "\n",
        "                z_split = tf.split(z, num_or_size_splits=[split_dim] * 5 + [split_dim_remainder], axis=-1)\n",
        "\n",
        "            else:\n",
        "                split_dim = self.z_dim // 6\n",
        "                split_dim_remainder = self.z_dim - (split_dim * 6)\n",
        "\n",
        "                if split_dim_remainder == 0 :\n",
        "                    z_split = tf.split(z, num_or_size_splits=[split_dim] * 6, axis=-1)\n",
        "                else :\n",
        "                    z_split = tf.split(z, num_or_size_splits=[split_dim] * 5 + [split_dim_remainder], axis=-1)\n",
        "\n",
        "\n",
        "            ch = 16 * self.ch\n",
        "            x = fully_conneted(z_split[0], units=4 * 4 * ch, sn=self.sn, scope='dense')\n",
        "            x = tf.reshape(x, shape=[-1, 4, 4, ch])\n",
        "\n",
        "            x = resblock_up_condition(x, z_split[1], channels=ch, use_bias=False, is_training=is_training, sn=self.sn, scope='resblock_up_16')\n",
        "            ch = ch // 2\n",
        "\n",
        "            x = resblock_up_condition(x, z_split[2], channels=ch, use_bias=False, is_training=is_training, sn=self.sn, scope='resblock_up_8')\n",
        "            ch = ch // 2\n",
        "\n",
        "            x = resblock_up_condition(x, z_split[3], channels=ch, use_bias=False, is_training=is_training, sn=self.sn, scope='resblock_up_4')\n",
        "            ch = ch // 2\n",
        "\n",
        "            x = resblock_up_condition(x, z_split[4], channels=ch, use_bias=False, is_training=is_training, sn=self.sn, scope='resblock_up_2')\n",
        "\n",
        "            # Non-Local Block\n",
        "            x = self_attention_2(x, channels=ch, sn=self.sn, scope='self_attention')\n",
        "            ch = ch // 2\n",
        "\n",
        "            x = resblock_up_condition(x, z_split[5], channels=ch, use_bias=False, is_training=is_training, sn=self.sn, scope='resblock_up_1')\n",
        "\n",
        "            x = batch_norm(x, is_training)\n",
        "            x = relu(x)\n",
        "            x = conv(x, channels=self.c_dim, kernel=3, stride=1, pad=1, use_bias=False, sn=self.sn, scope='G_logit')\n",
        "\n",
        "            x = tanh(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "    ##################################################################################\n",
        "    # Discriminator\n",
        "    ##################################################################################\n",
        "\n",
        "    def discriminator(self, x, is_training=True, reuse=False):\n",
        "        with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
        "            ch = self.ch\n",
        "\n",
        "            x = resblock_down(x, channels=ch, use_bias=False, is_training=is_training, sn=self.sn, scope='resblock_down_1')\n",
        "\n",
        "            # Non-Local Block\n",
        "            x = self_attention_2(x, channels=ch, sn=self.sn, scope='self_attention')\n",
        "            ch = ch * 2\n",
        "\n",
        "            x = resblock_down(x, channels=ch, use_bias=False, is_training=is_training, sn=self.sn, scope='resblock_down_2')\n",
        "            ch = ch * 2\n",
        "\n",
        "            x = resblock_down(x, channels=ch, use_bias=False, is_training=is_training, sn=self.sn, scope='resblock_down_4')\n",
        "            ch = ch * 2\n",
        "\n",
        "            x = resblock_down(x, channels=ch, use_bias=False, is_training=is_training, sn=self.sn, scope='resblock_down_8')\n",
        "            ch = ch * 2\n",
        "\n",
        "            x = resblock_down(x, channels=ch, use_bias=False, is_training=is_training, sn=self.sn, scope='resblock_down_16')\n",
        "\n",
        "            x = resblock(x, channels=ch, use_bias=False, is_training=is_training, sn=self.sn, scope='resblock')\n",
        "            x = relu(x)\n",
        "\n",
        "            x = global_sum_pooling(x)\n",
        "\n",
        "            x = fully_conneted(x, units=1, sn=self.sn, scope='D_logit')\n",
        "\n",
        "            return x\n",
        "\n",
        "    def gradient_penalty(self, real, fake):\n",
        "        if self.gan_type.__contains__('dragan'):\n",
        "            eps = tf.random_uniform(shape=tf.shape(real), minval=0., maxval=1.)\n",
        "            _, x_var = tf.nn.moments(real, axes=[0, 1, 2, 3])\n",
        "            x_std = tf.sqrt(x_var)  # magnitude of noise decides the size of local region\n",
        "\n",
        "            fake = real + 0.5 * x_std * eps\n",
        "\n",
        "        alpha = tf.random_uniform(shape=[self.batch_size, 1, 1, 1], minval=0., maxval=1.)\n",
        "        interpolated = real + alpha * (fake - real)\n",
        "\n",
        "        logit = self.discriminator(interpolated, reuse=True)\n",
        "\n",
        "        grad = tf.gradients(logit, interpolated)[0]  # gradient of D(interpolated)\n",
        "        grad_norm = tf.norm(flatten(grad), axis=1)  # l2 norm\n",
        "\n",
        "        GP = 0\n",
        "\n",
        "        # WGAN - LP\n",
        "        if self.gan_type == 'wgan-lp':\n",
        "            GP = self.ld * tf.reduce_mean(tf.square(tf.maximum(0.0, grad_norm - 1.)))\n",
        "\n",
        "        elif self.gan_type == 'wgan-gp' or self.gan_type == 'dragan':\n",
        "            GP = self.ld * tf.reduce_mean(tf.square(grad_norm - 1.))\n",
        "\n",
        "        return GP\n",
        "\n",
        "    ##################################################################################\n",
        "    # Model\n",
        "    ##################################################################################\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\" Graph Input \"\"\"\n",
        "        # images\n",
        "        Image_Data_Class = ImageData(self.img_size, self.c_dim, self.custom_dataset)\n",
        "        inputs = tf.data.Dataset.from_tensor_slices(self.data)\n",
        "\n",
        "        gpu_device = '/gpu:0'\n",
        "        inputs = inputs.\\\n",
        "            apply(shuffle_and_repeat(self.dataset_num)).\\\n",
        "            apply(map_and_batch(Image_Data_Class.image_processing, self.batch_size, num_parallel_batches=16, drop_remainder=True)).\\\n",
        "            apply(prefetch_to_device(gpu_device, self.batch_size))\n",
        "\n",
        "        inputs_iterator = inputs.make_one_shot_iterator()\n",
        "\n",
        "        self.inputs = inputs_iterator.get_next()\n",
        "\n",
        "        # noises\n",
        "        self.z = tf.truncated_normal(shape=[self.batch_size, 1, 1, self.z_dim], name='random_z')\n",
        "\n",
        "        \"\"\" Loss Function \"\"\"\n",
        "        # output of D for real images\n",
        "        real_logits = self.discriminator(self.inputs)\n",
        "\n",
        "        # output of D for fake images\n",
        "        fake_images = self.generator(self.z)\n",
        "        fake_logits = self.discriminator(fake_images, reuse=True)\n",
        "\n",
        "        if self.gan_type.__contains__('wgan') or self.gan_type == 'dragan':\n",
        "            GP = self.gradient_penalty(real=self.inputs, fake=fake_images)\n",
        "        else:\n",
        "            GP = 0\n",
        "\n",
        "        # get loss for discriminator\n",
        "        self.d_loss = discriminator_loss(self.gan_type, real=real_logits, fake=fake_logits) + GP\n",
        "\n",
        "        # get loss for generator\n",
        "        self.g_loss = generator_loss(self.gan_type, fake=fake_logits)\n",
        "\n",
        "        \"\"\" Training \"\"\"\n",
        "        # divide trainable variables into a group for D and a group for G\n",
        "        t_vars = tf.trainable_variables()\n",
        "        d_vars = [var for var in t_vars if 'discriminator' in var.name]\n",
        "        g_vars = [var for var in t_vars if 'generator' in var.name]\n",
        "\n",
        "        # optimizers\n",
        "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
        "            self.d_optim = tf.train.AdamOptimizer(self.d_learning_rate, beta1=self.beta1, beta2=self.beta2).minimize(self.d_loss, var_list=d_vars)\n",
        "\n",
        "            self.opt = MovingAverageOptimizer(tf.train.AdamOptimizer(self.g_learning_rate, beta1=self.beta1, beta2=self.beta2), average_decay=self.moving_decay)\n",
        "\n",
        "            self.g_optim = self.opt.minimize(self.g_loss, var_list=g_vars)\n",
        "\n",
        "        \"\"\"\" Testing \"\"\"\n",
        "        # for test\n",
        "        self.fake_images = self.generator(self.z, is_training=False, reuse=True)\n",
        "\n",
        "        \"\"\" Summary \"\"\"\n",
        "        self.d_sum = tf.summary.scalar(\"d_loss\", self.d_loss)\n",
        "        self.g_sum = tf.summary.scalar(\"g_loss\", self.g_loss)\n",
        "\n",
        "    ##################################################################################\n",
        "    # Train\n",
        "    ##################################################################################\n",
        "\n",
        "    def train(self):\n",
        "        # initialize all variables\n",
        "        tf.global_variables_initializer().run()\n",
        "\n",
        "        # saver to save model\n",
        "        self.saver = self.opt.swapping_saver()\n",
        "\n",
        "        # summary writer\n",
        "        self.writer = tf.summary.FileWriter(self.log_dir + '/' + self.model_dir, self.sess.graph)\n",
        "\n",
        "        # restore check-point if it exits\n",
        "        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
        "        if could_load:\n",
        "            start_epoch = (int)(checkpoint_counter / self.iteration)\n",
        "            start_batch_id = checkpoint_counter - start_epoch * self.iteration\n",
        "            counter = checkpoint_counter\n",
        "            print(\" [*] Load SUCCESS\")\n",
        "        else:\n",
        "            start_epoch = 0\n",
        "            start_batch_id = 0\n",
        "            counter = 1\n",
        "            print(\" [!] Load failed...\")\n",
        "\n",
        "        # loop for epoch\n",
        "        start_time = time.time()\n",
        "        past_g_loss = -1.\n",
        "        for epoch in range(start_epoch, self.epoch):\n",
        "            # get batch data\n",
        "            for idx in range(start_batch_id, self.iteration):\n",
        "\n",
        "                # update D network\n",
        "                _, summary_str, d_loss = self.sess.run([self.d_optim, self.d_sum, self.d_loss])\n",
        "                self.writer.add_summary(summary_str, counter)\n",
        "\n",
        "                # update G network\n",
        "                g_loss = None\n",
        "                if (counter - 1) % self.n_critic == 0:\n",
        "                    _, summary_str, g_loss = self.sess.run([self.g_optim, self.g_sum, self.g_loss])\n",
        "                    self.writer.add_summary(summary_str, counter)\n",
        "                    past_g_loss = g_loss\n",
        "\n",
        "                # display training status\n",
        "                counter += 1\n",
        "                if g_loss == None:\n",
        "                    g_loss = past_g_loss\n",
        "                print(\"Epoch: [%2d] [%5d/%5d] time: %4.4f, d_loss: %.8f, g_loss: %.8f\" \\\n",
        "                      % (epoch, idx, self.iteration, time.time() - start_time, d_loss, g_loss))\n",
        "\n",
        "                # save training results for every 300 steps\n",
        "                if np.mod(idx + 1, self.print_freq) == 0:\n",
        "                    samples = self.sess.run(self.fake_images)\n",
        "                    tot_num_samples = min(self.sample_num, self.batch_size)\n",
        "                    manifold_h = int(np.floor(np.sqrt(tot_num_samples)))\n",
        "                    manifold_w = int(np.floor(np.sqrt(tot_num_samples)))\n",
        "                    save_images(samples[:manifold_h * manifold_w, :, :, :],\n",
        "                                [manifold_h, manifold_w],\n",
        "                                './' + self.sample_dir + '/' + self.model_name + '_train_{:02d}_{:05d}.png'.format(\n",
        "                                    epoch, idx + 1))\n",
        "\n",
        "                if np.mod(idx + 1, self.save_freq) == 0:\n",
        "                    self.save(self.checkpoint_dir, counter)\n",
        "\n",
        "            # After an epoch, start_batch_id is set to zero\n",
        "            # non-zero value is only for the first epoch after loading pre-trained model\n",
        "            start_batch_id = 0\n",
        "\n",
        "            # save model\n",
        "            self.save(self.checkpoint_dir, counter)\n",
        "\n",
        "            # show temporal results\n",
        "            # self.visualize_results(epoch)\n",
        "\n",
        "        # save model for final step\n",
        "        self.save(self.checkpoint_dir, counter)\n",
        "\n",
        "    @property\n",
        "    def model_dir(self):\n",
        "        if self.sn :\n",
        "            sn = '_sn'\n",
        "        else :\n",
        "            sn = ''\n",
        "\n",
        "        return \"{}_{}_{}_{}_{}{}\".format(\n",
        "            self.model_name, self.dataset_name, self.gan_type, self.img_size, self.z_dim, sn)\n",
        "\n",
        "    def save(self, checkpoint_dir, step):\n",
        "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n",
        "\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "\n",
        "        self.saver.save(self.sess, os.path.join(checkpoint_dir, self.model_name + '.model'), global_step=step)\n",
        "\n",
        "    def load(self, checkpoint_dir):\n",
        "        print(\" [*] Reading checkpoints...\")\n",
        "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n",
        "\n",
        "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
        "        if ckpt and ckpt.model_checkpoint_path:\n",
        "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
        "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
        "            counter = int(ckpt_name.split('-')[-1])\n",
        "            print(\" [*] Success to read {}\".format(ckpt_name))\n",
        "            return True, counter\n",
        "        else:\n",
        "            print(\" [*] Failed to find a checkpoint\")\n",
        "            return False, 0\n",
        "\n",
        "    def visualize_results(self, epoch):\n",
        "        tot_num_samples = min(self.sample_num, self.batch_size)\n",
        "        image_frame_dim = int(np.floor(np.sqrt(tot_num_samples)))\n",
        "\n",
        "        \"\"\" random condition, random noise \"\"\"\n",
        "\n",
        "        samples = self.sess.run(self.fake_images)\n",
        "\n",
        "        save_images(samples[:image_frame_dim * image_frame_dim, :, :, :], [image_frame_dim, image_frame_dim],\n",
        "                    self.sample_dir + '/' + self.model_name + '_epoch%02d' % epoch + '_visualize.png')\n",
        "\n",
        "    def test(self):\n",
        "        tf.global_variables_initializer().run()\n",
        "\n",
        "        self.saver = tf.train.Saver()\n",
        "        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
        "        result_dir = os.path.join(self.result_dir, self.model_dir)\n",
        "        check_folder(result_dir)\n",
        "\n",
        "        if could_load:\n",
        "            print(\" [*] Load SUCCESS\")\n",
        "        else:\n",
        "            print(\" [!] Load failed...\")\n",
        "\n",
        "        tot_num_samples = min(self.sample_num, self.batch_size)\n",
        "        image_frame_dim = int(np.floor(np.sqrt(tot_num_samples)))\n",
        "\n",
        "        \"\"\" random condition, random noise \"\"\"\n",
        "\n",
        "        for i in range(self.test_num):\n",
        "            samples = self.sess.run(self.fake_images)\n",
        "\n",
        "            save_images(samples[:image_frame_dim * image_frame_dim, :, :, :],\n",
        "                        [image_frame_dim, image_frame_dim],\n",
        "                        result_dir + '/' + self.model_name + '_test_{}.png'.format(i))"
      ],
      "metadata": {
        "id": "i7osy4QOOOhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse"
      ],
      "metadata": {
        "id": "nhC4QFLUO3a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_args():\n",
        "    desc = \"Tensorflow implementation of BigGAN\"\n",
        "    parser = argparse.ArgumentParser(description=desc)\n",
        "    parser.add_argument('--phase', type=str, default='train', help='train or test ?')\n",
        "    parser.add_argument('--dataset', type=str, default='celebA-HQ', help='[mnist / cifar10 / custom_dataset]')\n",
        "\n",
        "    parser.add_argument('--epoch', type=int, default=50, help='The number of epochs to run')\n",
        "    parser.add_argument('--iteration', type=int, default=10000, help='The number of training iterations')\n",
        "    parser.add_argument('--batch_size', type=int, default=2048, help='The size of batch per gpu')\n",
        "    parser.add_argument('--ch', type=int, default=96, help='base channel number per layer')\n",
        "\n",
        "    # SAGAN\n",
        "    # batch_size = 256\n",
        "    # base channel = 64\n",
        "    # epoch = 100 (1M iterations)\n",
        "\n",
        "    parser.add_argument('--print_freq', type=int, default=1000, help='The number of image_print_freqy')\n",
        "    parser.add_argument('--save_freq', type=int, default=1000, help='The number of ckpt_save_freq')\n",
        "\n",
        "    parser.add_argument('--g_lr', type=float, default=0.00005, help='learning rate for generator')\n",
        "    parser.add_argument('--d_lr', type=float, default=0.0002, help='learning rate for discriminator')\n",
        "\n",
        "    # if lower batch size\n",
        "    # g_lr = 0.0001\n",
        "    # d_lr = 0.0004\n",
        "\n",
        "    # if larger batch size\n",
        "    # g_lr = 0.00005\n",
        "    # d_lr = 0.0002\n",
        "\n",
        "    parser.add_argument('--beta1', type=float, default=0.0, help='beta1 for Adam optimizer')\n",
        "    parser.add_argument('--beta2', type=float, default=0.9, help='beta2 for Adam optimizer')\n",
        "    parser.add_argument('--moving_decay', type=float, default=0.9999, help='moving average decay for generator')\n",
        "\n",
        "    parser.add_argument('--z_dim', type=int, default=128, help='Dimension of noise vector')\n",
        "    parser.add_argument('--sn', type=str2bool, default=True, help='using spectral norm')\n",
        "\n",
        "    parser.add_argument('--gan_type', type=str, default='hinge', help='[gan / lsgan / wgan-gp / wgan-lp / dragan / hinge]')\n",
        "    parser.add_argument('--ld', type=float, default=10.0, help='The gradient penalty lambda')\n",
        "\n",
        "    parser.add_argument('--n_critic', type=int, default=2, help='The number of critic')\n",
        "\n",
        "    parser.add_argument('--img_size', type=int, default=512, help='The size of image')\n",
        "    parser.add_argument('--sample_num', type=int, default=64, help='The number of sample images')\n",
        "\n",
        "    parser.add_argument('--test_num', type=int, default=10, help='The number of images generated by the test')\n",
        "\n",
        "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoint',\n",
        "                        help='Directory name to save the checkpoints')\n",
        "    parser.add_argument('--result_dir', type=str, default='results',\n",
        "                        help='Directory name to save the generated images')\n",
        "    parser.add_argument('--log_dir', type=str, default='logs',\n",
        "                        help='Directory name to save training logs')\n",
        "    parser.add_argument('--sample_dir', type=str, default='samples',\n",
        "                        help='Directory name to save the samples on training')\n",
        "\n",
        "    return check_args(parser.parse_args())\n",
        "\n",
        "\"\"\"checking arguments\"\"\"\n",
        "def check_args(args):\n",
        "    # --checkpoint_dir\n",
        "    check_folder(args.checkpoint_dir)\n",
        "\n",
        "    # --result_dir\n",
        "    check_folder(args.result_dir)\n",
        "\n",
        "    # --result_dir\n",
        "    check_folder(args.log_dir)\n",
        "\n",
        "    # --sample_dir\n",
        "    check_folder(args.sample_dir)\n",
        "\n",
        "    # --epoch\n",
        "    try:\n",
        "        assert args.epoch >= 1\n",
        "    except:\n",
        "        print('number of epochs must be larger than or equal to one')\n",
        "\n",
        "    # --batch_size\n",
        "    try:\n",
        "        assert args.batch_size >= 1\n",
        "    except:\n",
        "        print('batch size must be larger than or equal to one')\n",
        "    return args\n",
        "\n",
        "\n",
        "\"\"\"main\"\"\"\n",
        "def main():\n",
        "    # parse arguments\n",
        "    #args = parse_args()\n",
        "    args = \"--phase train --dataset celebA-HQ --gan_type hinge\"\n",
        "    if args is None:\n",
        "      exit()\n",
        "\n",
        "    # open session\n",
        "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
        "        # default gan = BigGAN_128\n",
        "\n",
        "        gan = BigGAN_128(sess, args)\n",
        "\n",
        "        # build graph\n",
        "        gan.build_model()\n",
        "\n",
        "        # show network architecture\n",
        "        show_all_variables()\n",
        "\n",
        "        if args.phase == 'train' :\n",
        "            # launch the graph in a session\n",
        "            gan.train()\n",
        "\n",
        "            # visualize learned generator\n",
        "            gan.visualize_results(args.epoch - 1)\n",
        "\n",
        "            print(\" [*] Training finished!\")\n",
        "\n",
        "        if args.phase == 'test' :\n",
        "            gan.test()\n",
        "            print(\" [*] Test finished!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "4fofFbBCOYCv",
        "outputId": "bfcbb43b-3bd1-4cd7-c984-a8cc4950da22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-ac6fdeb734c7>\u001b[0m in \u001b[0;36m<cell line: 120>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-ac6fdeb734c7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# open session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallow_soft_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;31m# default gan = BigGAN_128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
          ]
        }
      ]
    }
  ]
}